{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csc_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import h5py\n",
    "\n",
    "from sklearn import model_selection, feature_selection, kernel_approximation, ensemble, linear_model, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Import data\")\n",
    "\n",
    "X = pd.read_hdf('data_window.h5', key='data')\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X2 = pd.read_hdf('data_window3.h5', key='data')\n",
    "X2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X = X.join(X2)\n",
    "\n",
    "X.drop('window_id', axis=1, inplace=True)\n",
    "\n",
    "y = X['Label_<lambda>']\n",
    "X.drop('Label_<lambda>', axis=1, inplace=True)\n",
    "\n",
    "labels = np.load(\"data_window_labels.npy\")\n",
    "\n",
    "print(X.columns.values)\n",
    "print(labels)\n",
    "print(np.where(labels == 'flow=From-Botne')[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bin6 = y == np.where(labels == 'flow=From-Botne')[0][0]\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y_bin6, test_size=0.33, random_state=123456)\n",
    "\n",
    "print(\"y\", np.unique(y, return_counts=True))\n",
    "print(\"y_train\", np.unique(y_train, return_counts=True))\n",
    "print(\"y_test\", np.unique(y_test, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_svm_cross_validation(X, y, svc_args={'loss': 'hinge', 'penalty': 'elasticnet', 'max_iter': 1000, 'alpha': 1e-9, 'tol': 1e-3, 'random_state': 123456, 'class_weight': None}, kernel_args={'kernel': 'rbf', 'gamma': None, 'degree': None, 'n_components': 100, 'random_state': 123456}):\n",
    "    # print(\"kernel_approx\")\n",
    "    # feature_map_nystroem = kernel_approximation.Nystroem(**kernel_args)\n",
    "    # feature_map_nystroem.fit(X)\n",
    "    # X_new = feature_map_nystroem.transform(X)\n",
    "\n",
    "    print(\"SVM\")\n",
    "    clf = linear_model.SGDClassifier(**svc_args)\n",
    "    cv = model_selection.ShuffleSplit(\n",
    "        n_splits=10, test_size=0.1, random_state=123456)\n",
    "    scores = model_selection.cross_validate(clf, X, y, cv=cv, scoring=[\n",
    "                                            'precision', 'recall', 'f1'], return_train_score=True)\n",
    "    print(scores)\n",
    "    return [np.mean(scores['test_precision']), np.mean(scores['test_recall']), np.mean(scores['test_f1'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_scale = 1/X_train.shape[1]\n",
    "print(\"gamma_scale=\", gamma_scale)\n",
    "tab_gamma = np.concatenate((np.linspace(0.001, 0.04, 10), [gamma_scale]))\n",
    "print(tab_gamma)\n",
    "\n",
    "tab_score = np.array([apply_svm_cross_validation(X_train, y_train, kernel_args={\n",
    "                     'kernel': 'rbf', 'gamma': gamma, 'degree': None, 'n_components': 200, 'random_state': 123456}) for gamma in tab_gamma])\n",
    "print(tab_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tab_gamma, tab_score[:, 0])\n",
    "plt.plot(tab_gamma, tab_score[:, 1])\n",
    "plt.plot(tab_gamma, tab_score[:, 2])\n",
    "plt.legend([\"test_precision\", \"test_recall\", \"test_f1\"])\n",
    "plt.xlabel(\"Gamma\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.savefig(\"cross_validation_svm_gamma.pdf\", format=\"pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_degree = np.linspace(2, 4, 3)\n",
    "print(tab_degree)\n",
    "\n",
    "tab_score = np.array([apply_svm_cross_validation(X_train, y_train, kernel_args={\n",
    "                     'kernel': 'poly', 'gamma': None, 'degree': degree, 'n_components': 200, 'random_state': 123456}) for degree in tab_degree])\n",
    "print(tab_score)\n",
    "\n",
    "plt.plot(tab_degree, tab_score[:, 0])\n",
    "plt.plot(tab_degree, tab_score[:, 1])\n",
    "plt.plot(tab_degree, tab_score[:, 2])\n",
    "plt.legend([\"test_precision\", \"test_recall\", \"test_f1\"])\n",
    "plt.xlabel(\"Degree\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.savefig(\"cross_validation_svm_degree.pdf\", format=\"pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_penalty = ['l1', 'l2', 'elasticnet']\n",
    "print(tab_penalty)\n",
    "\n",
    "tab_score = np.array([apply_svm_cross_validation(X_train, y_train, {\n",
    "                     'loss': 'hinge', 'penalty': penalty, 'max_iter': 1000, 'alpha': 1e-9, 'tol': 1e-3, 'random_state': 123456, 'class_weight': None}) for penalty in tab_penalty])\n",
    "print(tab_score)\n",
    "\n",
    "plt.plot(tab_penalty, tab_score[:, 0])\n",
    "plt.plot(tab_penalty, tab_score[:, 1])\n",
    "plt.plot(tab_penalty, tab_score[:, 2])\n",
    "plt.legend([\"test_precision\", \"test_recall\", \"test_f1\"])\n",
    "plt.xlabel(\"Regularization Penalty\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.savefig(\"cross_validation_svm_penalty.pdf\", format=\"pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map_nystroem = kernel_approximation.Nystroem(\n",
    "    kernel='poly', gamma=None, degree=2, n_components=200, random_state=123456)\n",
    "feature_map_nystroem.fit(X_train)\n",
    "X_train_new = feature_map_nystroem.transform(X_train)\n",
    "X_test_new = feature_map_nystroem.transform(X_test)\n",
    "\n",
    "clf = linear_model.SGDClassifier(loss='hinge', penalty='l2', max_iter=1000,\n",
    "                                 alpha=1e-9, tol=1e-3, random_state=123456, class_weight=None, verbose=1)\n",
    "clf.fit(X_train_new, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train\")\n",
    "y_pred_train = clf.predict(X_train_new)\n",
    "print(\"accuracy score = \", metrics.balanced_accuracy_score(y_train, y_pred_train))\n",
    "precision, recall, fbeta_score, support = metrics.precision_recall_fscore_support(\n",
    "    y_train, y_pred_train)\n",
    "print(\"precision = \", precision[1])\n",
    "print(\"recall = \", recall[1])\n",
    "print(\"fbeta_score = \", fbeta_score[1])\n",
    "print(\"support = \", support[1])\n",
    "\n",
    "print(\"Test\")\n",
    "y_pred_test = clf.predict(X_test_new)\n",
    "print(\"accuracy score = \", metrics.balanced_accuracy_score(y_test, y_pred_test))\n",
    "precision, recall, fbeta_score, support = metrics.precision_recall_fscore_support(\n",
    "    y_test, y_pred_test)\n",
    "print(\"precision = \", precision[1])\n",
    "print(\"recall = \", recall[1])\n",
    "print(\"fbeta_score = \", fbeta_score[1])\n",
    "print(\"support = \", support[1])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
